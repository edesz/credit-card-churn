{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0ec243-5977-4e55-9a27-ad27ad445a82",
   "metadata": {},
   "source": [
    "# Get At-Risk Customers Using Business Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7669f24-ce3e-444b-a556-ab011348e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from io import BytesIO, StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import botocore.exceptions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa48a3-f589-45a8-8c97-cc68d0298380",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d63c34-fc13-4991-a13d-ba93d3e15f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert load_dotenv(dotenv_path=PROJ_ROOT.parent / '.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d5022-cf30-4c71-a0ba-e4cc37708dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc_churn.costs as costs\n",
    "import cc_churn.visualization as vzu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b2c67f-08e4-4721-abd8-730f6bcd498c",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546ecec-c661-43e9-ba91-40e716039a74",
   "metadata": {},
   "source": [
    "Get the at-risk customers and determine how many customers should be selected in order to maximize true ROI while minimizing error in predicted ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9032175-6c5b-4d3d-acce-a891c3752de5",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9db5e8-5055-42ae-96f8-fb05b4111cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 data bucket details\n",
    "bucket_name = 'cc-churn-splits'\n",
    "# # name of validation data with predictions key (file) in private R2 bucket\n",
    "r2_key_val_partial = 'validation_predictions__logisticregression__'\n",
    "# # name of validation data with predictions key (file) in private R2 bucket\n",
    "r2_key_test_partial = 'test_predictions__logisticregression__'\n",
    "\n",
    "# columns to load\n",
    "columns = [\n",
    "    'clientnum',\n",
    "    'card_category',\n",
    "    'total_revolv_bal',\n",
    "    'total_trans_amt',\n",
    "    'model_name',\n",
    "    'y_pred_proba',\n",
    "    'y_pred',\n",
    "    'best_decision_threshold',\n",
    "    'is_churned',\n",
    "]\n",
    "\n",
    "# costs\n",
    "# # revenue from transactions (bank earns #% of transaction volume)\n",
    "interchange_rate = 0.02\n",
    "# # revenue from revolving balance (~20% interest)\n",
    "apr = 0.18\n",
    "# # fee revenue from credit card exposure (modeled from card type)\n",
    "card_fees = {\"Blue\": 0, \"Silver\": 50, \"Gold\": 100, \"Platinum\": 200}\n",
    "tenure_years = 3\n",
    "discount = 0.9\n",
    "# # percentage of churners who can be convinced to stay (i.e. success rate\n",
    "# # of saving a churning customer)\n",
    "success_rate = 0.40\n",
    "# # cost of intervention to get a single customer to not churn (discounts,\n",
    "# # call center time, retention offers, etc.)\n",
    "intervention_cost = 50\n",
    "# # cost of acquiring a new customer (Customer Acquisition Cost, CAC)\n",
    "replacement_cost = 200\n",
    "# # maximum number of customers that can be targeted based on client's budget\n",
    "num_customers_max = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dff478-e44b-47dc-bf9f-96d8d9b0fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = os.getenv('ACCOUNT_ID')\n",
    "access_key_id = os.getenv('ACCESS_KEY_ID')\n",
    "secret_access_key = os.getenv('SECRET_ACCESS_KEY')\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=f'https://{account_id}.r2.cloudflarestorage.com',\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    region_name='auto'\n",
    ")\n",
    "\n",
    "# costs\n",
    "multiplier = (1 - discount**tenure_years) / (1 - discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b34726-829f-447a-bd68-c175df46f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_read_parquet_r2(bucket_name, r2_key, columns):\n",
    "    \"\"\"Read parquet file from private R2 bucket.\"\"\"\n",
    "    s3_object = s3_client.get_object(Bucket=bucket_name, Key=r2_key)\n",
    "    df = pd.read_parquet(\n",
    "        BytesIO(s3_object['Body'].read()),\n",
    "        columns=columns,\n",
    "        dtype_backend='pyarrow',\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def pandas_read_filtered_parquets_r2(bucket_name, key_prefix, cols_to_load):\n",
    "    \"\"\"Read parquet files using partial filename from private R2 bucket.\"\"\"\n",
    "    s3_objects = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name, Prefix=key_prefix, MaxKeys=1\n",
    "    )\n",
    "    assert s3_objects['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            pandas_read_parquet_r2(\n",
    "                bucket_name, obj['Key'], columns=cols_to_load\n",
    "            )\n",
    "            for obj in s3_objects['Contents']\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_df_to_r2(df, bucket_name, r2_key):\n",
    "    \"\"\"Export DataFrame to file in private R2 bucket, if not present.\"\"\"\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket_name, Key=r2_key)\n",
    "        print(f\"Key {r2_key} already exists in bucket {bucket_name}\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"Key {r2_key} does not exist in bucket {bucket_name}\")\n",
    "            buffer = BytesIO()\n",
    "            df.to_parquet(\n",
    "                buffer,\n",
    "                index=False,\n",
    "                engine='pyarrow',\n",
    "                compression='gzip',\n",
    "            )\n",
    "            response = s3_client.put_object(\n",
    "                Bucket=bucket_name, Key=r2_key, Body=buffer.getvalue()\n",
    "            )\n",
    "            assert response['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "            print(f\"Exported {len(df):,} rows to key: {r2_key}\")\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"403\":\n",
    "            print(f\"Access denied to bucket {bucket_name} or key {r2_key}\")\n",
    "        else:\n",
    "            print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf91534-0391-43d9-abce-ef72c98ddbbe",
   "metadata": {},
   "source": [
    "## Load Data with Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbe11a-c0e9-4763-b3d2-40f2a843ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.ensemble as skens\n",
    "import sklearn.metrics as mtr\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.utils as skut\n",
    "from IPython.display import display\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# R2 data bucket details\n",
    "bucket_name = \"cc-churn-splits\"\n",
    "\n",
    "# columns to load\n",
    "columns = [\n",
    "    \"clientnum\",\n",
    "    \"card_category\",\n",
    "    \"total_revolv_bal\",\n",
    "    \"total_trans_amt\",\n",
    "    \"model_name\",\n",
    "    \"y_pred_proba\",\n",
    "    \"y_pred\",\n",
    "    \"best_decision_threshold\",\n",
    "    \"is_churned\",\n",
    "]\n",
    "\n",
    "ordinal_features = [\n",
    "    \"income_category\",\n",
    "    \"education_level\",\n",
    "]\n",
    "categorical_features = [\n",
    "    # 'card_category',\n",
    "    \"marital_status\",\n",
    "]\n",
    "numeric_features = [\n",
    "    # 'customer_age',\n",
    "    # # 'dependent_count',\n",
    "    \"months_on_book\",\n",
    "    \"num_products\",\n",
    "    \"months_inactive_12_mon\",\n",
    "    \"contacts_count_12_mon\",\n",
    "    \"total_revolv_bal\",\n",
    "    # 'avg_open_to_buy',\n",
    "    \"total_amt_chng_q4_q1\",\n",
    "    # 'total_trans_amt',\n",
    "    \"total_trans_ct\",\n",
    "    \"total_ct_chng_q4_q1\",\n",
    "    # 'avg_utilization_ratio',\n",
    "]\n",
    "features = numeric_features + ordinal_features + categorical_features\n",
    "\n",
    "\n",
    "def pandas_read_parquet_r2_v2(s3_client, bucket_name, r2_key, columns):\n",
    "    \"\"\"Read parquet file from private R2 bucket.\"\"\"\n",
    "    s3_object = s3_client.get_object(Bucket=bucket_name, Key=r2_key)\n",
    "    df = pd.read_parquet(\n",
    "        BytesIO(s3_object[\"Body\"].read()),\n",
    "        columns=columns,\n",
    "        dtype_backend=\"pyarrow\",\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = pandas_read_parquet_r2_v2(\n",
    "    s3_client, bucket_name, \"train_data.parquet.gzip\", None\n",
    ")\n",
    "df_val = pandas_read_parquet_r2_v2(\n",
    "    s3_client, bucket_name, \"validation_data.parquet.gzip\", None\n",
    ")\n",
    "df_test = pandas_read_parquet_r2_v2(\n",
    "    s3_client, bucket_name, \"test_data.parquet.gzip\", None\n",
    ")\n",
    "df_train_val = pd.concat([df_train, df_val])\n",
    "\n",
    "X_train = df_train.drop(columns=[\"is_churned\"])\n",
    "X_val = df_val.drop(columns=[\"is_churned\"])\n",
    "X_train_val = df_train_val.drop(columns=[\"is_churned\"])\n",
    "X_test = df_test.drop(columns=[\"is_churned\"])\n",
    "\n",
    "y_train = df_train[\"is_churned\"]\n",
    "y_val = df_val[\"is_churned\"]\n",
    "y_train_val = df_train_val[\"is_churned\"]\n",
    "y_test = df_test[\"is_churned\"]\n",
    "\n",
    "df = pd.concat([df_train, df_val, df_test])\n",
    "X = df.drop(columns=[\"is_churned\"])\n",
    "y = df[\"is_churned\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", pp.MinMaxScaler())])\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[(\"ohe\", pp.OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\"))]\n",
    ")\n",
    "ordinal_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"oe\",\n",
    "            pp.OrdinalEncoder(\n",
    "                categories=[\n",
    "                    [\n",
    "                        \"Unknown\",\n",
    "                        \"Less than $40K\",\n",
    "                        \"$40K - $60K\",\n",
    "                        \"$60K - $80K\",\n",
    "                        \"$80K - $120K\",\n",
    "                        \"$120K +\",\n",
    "                    ],\n",
    "                    [\n",
    "                        \"Unknown\",\n",
    "                        \"Uneducated\",\n",
    "                        \"High School\",\n",
    "                        \"College\",\n",
    "                        \"Graduate\",\n",
    "                        \"Post-Graduate\",\n",
    "                        \"Doctorate\",\n",
    "                    ],\n",
    "                ],\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                dtype=np.float64,\n",
    "                unknown_value=np.nan,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"ord\", ordinal_transformer, ordinal_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# clf = LogisticRegression(\n",
    "#     class_weight='balanced', random_state=42, n_jobs=-1\n",
    "# )\n",
    "clf = skens.HistGradientBoostingClassifier(\n",
    "    # # VERSION 1\n",
    "    # max_depth=3,\n",
    "    # max_bins=255,\n",
    "    # l2_regularization=0.25,\n",
    "    # learning_rate=0.1,\n",
    "    # max_iter=250,\n",
    "    # class_weight='balanced',\n",
    "    # random_state=42,\n",
    "    # VERSION 2\n",
    "    max_depth=3,\n",
    "    l2_regularization=0.25,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    ")\n",
    "# clf = skens.RandomForestClassifier(\n",
    "#     n_estimators=600,\n",
    "#     max_depth=3,\n",
    "#     class_weight='balanced',\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
    "\n",
    "best_decision_threshold = 0.5\n",
    "\n",
    "pipe.fit(X_train[features], y_train)\n",
    "y_val_pred_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_proba = pd.Series(\n",
    "    y_val_pred_proba, name=\"y_pred_proba\", index=X_val.index\n",
    ")\n",
    "y_val_pred = (\n",
    "    (y_val_pred_proba >= best_decision_threshold).astype(int).rename(\"y_pred\")\n",
    ")\n",
    "\n",
    "pipe.fit(X_train_val[features], y_train_val)\n",
    "y_train_val_pred_proba = pipe.predict_proba(X_train_val)[:, 1]\n",
    "y_train_val_pred_proba = pd.Series(\n",
    "    y_train_val_pred_proba, name=\"y_pred_proba\", index=X_train_val.index\n",
    ")\n",
    "y_train_val_pred = (\n",
    "    (y_train_val_pred_proba >= best_decision_threshold)\n",
    "    .astype(int)\n",
    "    .rename(\"y_pred\")\n",
    ")\n",
    "\n",
    "y_test_pred_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "y_test_pred_proba = pd.Series(\n",
    "    y_test_pred_proba, name=\"y_pred_proba\", index=X_test.index\n",
    ")\n",
    "y_test_pred = (\n",
    "    (y_test_pred_proba >= best_decision_threshold).astype(int).rename(\"y_pred\")\n",
    ")\n",
    "\n",
    "y_pred_proba = pipe.predict_proba(X)[:, 1]\n",
    "y_pred_proba = pd.Series(y_pred_proba, name=\"y_pred_proba\", index=X.index)\n",
    "y_pred = (y_pred_proba >= best_decision_threshold).astype(int).rename(\"y_pred\")\n",
    "\n",
    "precision, recall, thresholds = mtr.precision_recall_curve(\n",
    "    y_train_val, y_train_val_pred\n",
    ")\n",
    "df_metrics_train_val = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"recall\": mtr.recall_score(y_train_val, y_train_val_pred),\n",
    "            \"f2_score\": mtr.fbeta_score(y_train_val, y_train_val_pred, beta=2),\n",
    "            \"roc_auc\": mtr.roc_auc_score(y_train_val, y_train_val_pred),\n",
    "            \"prauc\": mtr.auc(recall, precision),\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .transpose()\n",
    "    .assign(split=\"train+val\")\n",
    ")\n",
    "\n",
    "precision, recall, thresholds = mtr.precision_recall_curve(y_test, y_test_pred)\n",
    "df_metrics_test = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"recall\": mtr.recall_score(y_test, y_test_pred),\n",
    "            \"f2_score\": mtr.fbeta_score(y_test, y_test_pred, beta=2),\n",
    "            \"roc_auc\": mtr.roc_auc_score(y_test, y_test_pred),\n",
    "            \"prauc\": mtr.auc(recall, precision),\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .transpose()\n",
    "    .assign(split=\"test\")\n",
    ")\n",
    "\n",
    "metrics_list = [\"prauc\", \"f2\", \"recall\", \"rocauc\"]\n",
    "df_metrics = (\n",
    "    pd.concat([df_metrics_train_val, df_metrics_test], ignore_index=True)\n",
    "    .assign(model_name=type(clf).__name__)\n",
    "    .pivot(index=\"model_name\", columns=[\"split\"])\n",
    "    .set_axis(\n",
    "        [\n",
    "            \"test_recall\",\n",
    "            \"train_recall\",\n",
    "            \"test_f2\",\n",
    "            \"train_f2\",\n",
    "            \"test_rocauc\",\n",
    "            \"train_rocauc\",\n",
    "            \"test_prauc\",\n",
    "            \"train_prauc\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    .reset_index()\n",
    "    .style.set_properties(\n",
    "        subset=[\"model_name\"] + [f\"test_{m}\" for m in metrics_list],\n",
    "        **{\"background-color\": \"yellow\", \"color\": \"black\"},\n",
    "    )\n",
    ")\n",
    "display(df_metrics)\n",
    "\n",
    "df_val_pred = pd.concat([df_val, y_val_pred_proba, y_val_pred], axis=1).assign(\n",
    "    model_name=type(clf).__name__,\n",
    "    best_decision_threshold=best_decision_threshold,\n",
    ")[columns]\n",
    "assert df_val_pred.isna().sum().sum() == 0\n",
    "\n",
    "df_test_pred = pd.concat(\n",
    "    [df_test, y_test_pred_proba, y_test_pred], axis=1\n",
    ").assign(\n",
    "    model_name=type(clf).__name__,\n",
    "    best_decision_threshold=best_decision_threshold,\n",
    ")[\n",
    "    columns\n",
    "]\n",
    "assert df_test_pred.isna().sum().sum() == 0\n",
    "\n",
    "df_all_pred = pd.concat([df, y_pred_proba, y_pred], axis=1).assign(\n",
    "    model_name=type(clf).__name__,\n",
    "    best_decision_threshold=best_decision_threshold,\n",
    ")[columns]\n",
    "assert df_all_pred.isna().sum().sum() == 0\n",
    "\n",
    "print(\n",
    "    f\"Size of validation data = {len(df_val_pred):,} rows X \"\n",
    "    f\"{df_val_pred.shape[1]:,} columns\"\n",
    ")\n",
    "print(\n",
    "    f\"Size of test data = {len(df_test_pred):,} rows X \"\n",
    "    f\"{df_test_pred.shape[1]:,} columns\"\n",
    ")\n",
    "print(\n",
    "    f\"Size of all data = {len(df_all_pred):,} rows X \"\n",
    "    f\"{df_all_pred.shape[1]:,} columns\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3e06-4997-4667-9c32-d17750ed94eb",
   "metadata": {},
   "source": [
    "Load validation data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a228b7-fbb1-4479-98eb-60ec4403f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df_val_pred = pandas_read_filtered_parquets_r2(\n",
    "#     bucket_name, r2_key_val_partial, columns\n",
    "# )\n",
    "print(f\"Got {len(df_val_pred):,} rows of validation split predictions\")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92182c86-3c82-47e1-bcbd-ae6ed877ab89",
   "metadata": {},
   "source": [
    "Load test data with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfc624-69c2-49f4-b446-f1f348a5e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df_test_pred = pandas_read_filtered_parquets_r2(\n",
    "#     bucket_name, r2_key_test_partial, columns\n",
    "# )\n",
    "print(f\"Got {len(df_test_pred):,} rows of test split predictions\")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b536c7-5a12-4e24-8929-a40b9a5240b5",
   "metadata": {},
   "source": [
    "Extract best decision threshold from model predictions of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d47d4-4a2f-4041-bb70-71d61f617587",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_decision_threshold = df_val_pred['best_decision_threshold'].head(1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82ddf1-6d7a-4748-a3c1-e07add7ba243",
   "metadata": {},
   "source": [
    "Extract name of best ML model from model predictions of the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b7c78c-adae-441c-99e2-d33771be9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = df_val_pred['model_name'].head(1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc15f9-9dd1-452e-b2c2-9c8b41a0e163",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257665a-c38e-4365-99ed-bf5f1063c989",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04204ace-1bf9-4bac-bc3e-0340c42150dd",
   "metadata": {},
   "source": [
    "Calculate the true savings, expected (predicted) savings and error in predicted savings (cost) using the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ad172-f34d-465a-8daf-15364a6e3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_val, _, _ = costs.get_cost(\n",
    "    df_val_pred,\n",
    "    best_decision_threshold,\n",
    "    interchange_rate,\n",
    "    apr,\n",
    "    card_fees,\n",
    "    multiplier,\n",
    "    success_rate,\n",
    "    intervention_cost,\n",
    ")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_costs_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609cb12-3cca-488a-865d-f8eff6a2224b",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. The following columns are related to the business costs and are discussed in the [project scoping document](https://github.com/edesz/credit-card-churn/blob/main/references/01_proposal.md)\n",
    "   - `interchange_rev`\n",
    "   - `interest_rev`\n",
    "   - `fee_rev`\n",
    "   - `annual_rev`\n",
    "   - `clv`\n",
    "   - `success_rate`\n",
    "   - `expected_savings`\n",
    "   - `true_savings`\n",
    "   - `cum_pred_savings`\n",
    "   - `cum_true_savings`\n",
    "   - `n`\n",
    "   - `total_intervention_cost`\n",
    "   - `ROI`\n",
    "   - `ROI_pred`\n",
    "   - `ROI_error`\n",
    "   - `ROI_percent`\n",
    "   - `ROI_percent_pred`\n",
    "2. The costs are only calculated for customers that are predicted to churn since these are the customers that will be targeted. For this reason, there are fewer rows in the costs `DataFrame` (`df_costs_val`) than in the `DataFrame` with the ML model predictions for the validation data (`df_val_pred`).\n",
    "3. ROI is a business-focused Key Performance Indicator (KPI). It provides a broader view of the financial efficiency of an entire investment, considering both costs and gains. So, the KPI that should be reported to the client is the expected (predicted) ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c67570-4016-4e9b-875f-83e2225ae64a",
   "metadata": {},
   "source": [
    "### Get Optimal Number of Targeted Customers (`N`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651514e-122d-4f85-9e6d-6273e4b240a8",
   "metadata": {},
   "source": [
    "Plot true and predicted expected savings and ROI curves to visualize the following\n",
    "\n",
    "1. true ROI\n",
    "2. predicted ROI\n",
    "\n",
    "using the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4848d8-6e39-4069-a480-95b59a08ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vzu.plot_roi_curves(\n",
    "    df_costs_val['n'],\n",
    "    df_costs_val['cum_true_savings'],\n",
    "    df_costs_val['cum_pred_savings'],\n",
    "    df_costs_val['ROI_percent'],\n",
    "    df_costs_val['ROI_percent_pred'],\n",
    "    {},\n",
    "    ptitle=(\n",
    "        'Excluding initial Noisy Period, ROI is Maximized after Selecting Top '\n",
    "        '~190 At-Risk Customers'\n",
    "    ),\n",
    "    legend_loc='lower right',\n",
    "    xlabel=f\"Number of Predicted Churners to Contact (Top-N)\",\n",
    "    ylabel=\"Expected Net Savings ($)\",\n",
    "    fig_size=(12, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49bdee0-2e7b-4787-a228-2a61e613a8e9",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. The model cost is the error in predicted ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53884ff6-d642-420b-8101-4e20318c7fc6",
   "metadata": {},
   "source": [
    "Below are selected components involved in the calculation of ROI to explore the source of the initial noisy period in the ROI from the above chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fdc502-7542-470f-95bc-930121b6f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_table = [\n",
    "    'n',\n",
    "    'y_pred_proba',\n",
    "    'total_revolv_bal',\n",
    "    'interchange_rev',\n",
    "    'interest_rev',\n",
    "    'fee_rev',\n",
    "    'expected_savings',\n",
    "    'ROI_percent',\n",
    "]\n",
    "with pd.option_context('display.max_rows', None):\n",
    "    display(\n",
    "        df_costs_val[cols_table]\n",
    "        .head(25)\n",
    "        .style\n",
    "        .set_properties(\n",
    "            subset=[\n",
    "                'total_revolv_bal',\n",
    "                'interchange_rev',\n",
    "                'interest_rev',\n",
    "                'expected_savings',\n",
    "                'ROI_percent',\n",
    "            ],\n",
    "            **{'background-color': 'yellow', 'color': 'black'}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373b7ef-03e8-48a4-ab82-ddda221f02b2",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "1. High fluctuations are caused by customers with a high `total_revolv_bal`. These customers generate higher `interest_rev` for the client than other customers. The `interest_rev` term dominates the calculation of ROI, which is not the case for other customers. As a result, ROI fluctuates strongly when these customers are included. Selecting as many **high `total_revolv_bal`** customers as possible captures steep increases in ROI.\n",
    "2. `fee_rev` is fixed based on the `card_category` column. So, the interest revenue can dominate the overall ROI if the customer has a high revolving balance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06371f-e484-4ca6-91ac-d31d701bf613",
   "metadata": {},
   "source": [
    "**Observations from the chart**\n",
    "\n",
    "1. There are customers with a high `total_revolv_bal` in the top 75 customers. This results in the maximum possible ROI. Between ~75 and ~210, the influence of these customers is minimal so strong fluctuations are not observed. Between ~210 and ~245, the high `total_revolv_bal` customers appear again. After selecting the top ~245 customers, ROI shows a weak downward trend.\n",
    "2. Depending on the client's available budget, the second increase resulting in a ROI peak at ~245 customers should be selected. There are two possible scenarios\n",
    "   - if the budget allows for targeting at most the top 100 customers then the optimal number of customers is ~35\n",
    "   - if there is room in the budget to target all possible at-risk cutomers then the optimal number of customers is ~245\n",
    "\n",
    "   Here, we will assume the latter is true. So, the optimal number customers to be targeted is ~245."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b70e9c-ab66-4500-8786-46a5d8cee2a4",
   "metadata": {},
   "source": [
    "Find the optimal number of customers to target in order to minimize error in predicted ROI and maximize true ROI, using the costs on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82accd-136d-450a-ad36-0acf9b7dfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_optimal = (\n",
    "    df_costs_val\n",
    "    .query(\n",
    "        \"(total_intervention_cost > 0) & \"\n",
    "        # capture second peak in ROI\n",
    "        \"(n >= 200) & (n < 250)\"\n",
    "    )\n",
    "    .sort_values(\n",
    "        by=['ROI', 'ROI_error', 'n'], ascending=[False, True, True],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    .head(1)\n",
    ")\n",
    "optimal_N_roi = df_costs_optimal['n'].squeeze()\n",
    "cols_costs = [\n",
    "    'n',\n",
    "    'cum_true_savings',\n",
    "    'cum_pred_savings',\n",
    "    'ROI_error',\n",
    "    'ROI_percent',\n",
    "    'ROI_percent_pred',\n",
    "]\n",
    "(\n",
    "    df_costs_optimal[cols_costs]\n",
    "    .style\n",
    "    .set_properties(\n",
    "        subset=['ROI_error', 'ROI_percent_pred'],\n",
    "        **{'background-color': 'yellow', 'color': 'black'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22caafca-4ec3-4832-a4f0-d0cc3970eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_error_optimal_val = df_costs_optimal['ROI_error'].squeeze()\n",
    "predicted_roi_optimal_val = df_costs_optimal['ROI_percent_pred'].squeeze()\n",
    "Markdown(\n",
    "    \"**Observations**\\n\"\n",
    "    \"1. In order to maximize true ROI and minimize the error in predicted \"\n",
    "    f\"ROI, the optimal number of customers to target is {optimal_N_roi}. \"\n",
    "    \"This is consistent with observations from the chart above.\\n\"\n",
    "    f\"2. If the top {optimal_N_roi} customers from the test data are \"\n",
    "    \"targeted, then the\\n\"\n",
    "    \"   - error in the predicted ROI is approximately \"\n",
    "    f\"{roi_error_optimal_val:.1f}%\\n\"\n",
    "    f\"   - predicted ROI is approximately {predicted_roi_optimal_val:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31d723-3a40-40eb-a9d0-8121541020b0",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c8252-2950-453c-a85f-f4a93ceb5004",
   "metadata": {},
   "source": [
    "### Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d39cad-a46e-47a4-b9cb-58184188bc22",
   "metadata": {},
   "source": [
    "Get the true and predicted class imbalance for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb7f2a-b1c5-405b-9119-ae465b2aaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_true_pred_class_imbalance = (\n",
    "    (\n",
    "        df_test_pred['y_pred']\n",
    "        .value_counts(normalize=True)\n",
    "        .rename('predicted')\n",
    "        .to_frame()\n",
    "    )\n",
    "    .merge(\n",
    "        (\n",
    "            df_test_pred['is_churned']\n",
    "            .value_counts(normalize=True)\n",
    "            .rename('true')\n",
    "            .to_frame()\n",
    "        ),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "    )\n",
    ")\n",
    "df_true_pred_class_imbalance.index = df_true_pred_class_imbalance.index.map(\n",
    "    {0: 'No Churn', 1: 'Churn'}\n",
    ")\n",
    "churn_true = df_true_pred_class_imbalance.loc['Churn']['true']\n",
    "churn_pred = df_true_pred_class_imbalance.loc['Churn']['predicted']\n",
    "df_true_pred_class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539b84f-c91c-4a19-b222-4162dd8cca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    \"**Observations**\\n\"\n",
    "    f\"1. The class imbalance in the test split is approximately the same as that \"\n",
    "    f\"in the training split, which was seen in the EDA notebook. \"\n",
    "    f\"~{100*churn_true:.2f}% of customers showed churn in the test data.\\n\"\n",
    "    f\"2. Due to the inaccuracy of the model, ~{churn_pred:.2f}% instead of \"\n",
    "    f\"~{churn_pred:.2f}% of customers are predicted to churn.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba980b-5094-4864-b169-9b51defc6a22",
   "metadata": {},
   "source": [
    "The class imbalance in the validation and test data is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134e13b-3e71-49b1-a6d4-4ff6115546c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_val_test_true_class_imbalance = (\n",
    "    df_val_pred['is_churned']\n",
    "    .value_counts(normalize=True)\n",
    "    .rename('validation')\n",
    "    .to_frame()\n",
    "    .merge(\n",
    "        df_test_pred['is_churned'].value_counts(normalize=True).rename('test').to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left',\n",
    "    )\n",
    ")\n",
    "df_val_test_true_class_imbalance.index = df_val_test_true_class_imbalance.index.map(\n",
    "    {0: 'No Churn', 1: 'Churn'}\n",
    ")\n",
    "df_val_test_true_class_imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c09d8-3a74-4a06-9140-c79d6e1c2b9d",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. It is reassuring that the class imbalances are nearly equivalent between the validation and test splits. Patterns seen in during model validation should not be impacted by changes of the class imbalance during model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b40ea-fe44-4bce-a761-b1fef79c732f",
   "metadata": {},
   "source": [
    "Show the class imbalance and distribution of prediction probabilities for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e60a2c8-f01b-4bbe-b7e3-876364c58182",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vzu.plot_class_imbalance_proba_distribution(\n",
    "    df_clasS_imbalance=df_true_pred_class_imbalance.rename(columns=str.title),\n",
    "    df_probabilities=(df_test_pred['y_pred_proba']*100),\n",
    "    ptitle1='~5% Higher Churn Predicted in Test Split',\n",
    "    title1_xloc=-0.3,\n",
    "    ptitle2=(\n",
    "        'Predicted Probabilities show Right Skew with Weak Peak Above ~90%'\n",
    "    ),\n",
    "    vline_label=f'Optimized Churn Cutoff ({best_decision_threshold*100:.0f}%)',\n",
    "    decision_threshold=best_decision_threshold,\n",
    "    subfigure_width_ratios=[1.15, 3],\n",
    "    fig_size=(12, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680e6799-b35b-473e-97cf-b36f8c4fedfa",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "1. As expected from the predicted class imbalance, the distribution of predicted probabilities is right-skewed and a small fraction of customers have a predicted probability above 50% (the tuned classification decision threshold)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712da95-1482-4958-971e-ab09ddb5d092",
   "metadata": {},
   "source": [
    "### Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93df97-e861-4739-90e8-8a25e6e8275d",
   "metadata": {},
   "source": [
    "Calculate the true savings, expected (predicted) savings and error in predicted savings (cost) using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ac03d-7ad1-48b8-be0e-2a861b2684d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_test, _, _ = costs.get_cost(\n",
    "    df_test_pred,\n",
    "    best_decision_threshold,\n",
    "    interchange_rate,\n",
    "    apr,\n",
    "    card_fees,\n",
    "    multiplier,\n",
    "    success_rate,\n",
    "    intervention_cost,\n",
    ")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_costs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25a66d-6a9f-4a12-b546-7028456b1721",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. The column names are identical to those in the costs calculated using the validation data (`df_costs_val`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa61b361-c702-4cbf-9f1e-e0744b359998",
   "metadata": {},
   "source": [
    "### Impact of Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc01771-5baa-4a00-9844-1f90e27c3a83",
   "metadata": {},
   "source": [
    "The total impact of churn is estimated below for test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02609efe-c134-4442-9fc4-ea55d50f8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_churn_impact_test = (\n",
    "    df_costs_test\n",
    "    .groupby('is_churned')\n",
    "    .agg({'clv': ['sum', 'count']})\n",
    "    .set_axis(['clv_total', 'num_customers'], axis=1)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        clv_fraction=lambda df: (\n",
    "            (df['clv_total']/df['clv_total'].sum()).mul(100)\n",
    "        ),\n",
    "        clv_per_customer=lambda df: df['clv_total']/df['num_customers'],\n",
    "    )\n",
    "    .query(\"is_churned == True\")\n",
    "    .reset_index(drop=True)\n",
    "    .assign(\n",
    "        impact_per_customer=lambda df: df['clv_per_customer']+replacement_cost,\n",
    "        impact_total=lambda df: (\n",
    "            df['clv_total']+df['num_customers'].mul(replacement_cost)\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "impact = df_churn_impact_test['impact_total'].squeeze()\n",
    "impact_per_cust = df_churn_impact_test['impact_per_customer'].squeeze()\n",
    "frac_clv_lost = df_churn_impact_test['clv_fraction'].squeeze()\n",
    "clv_per_customer_lost = df_churn_impact_test['clv_per_customer'].squeeze()\n",
    "df_churn_impact_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb430f92-0ebe-48a8-ab9f-5250b2498b07",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. As per the assumption made in the scoping noteboo, the CAC is assumed to be $200 CAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ec46b-7c56-4cd2-8802-b457a02026b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    \"**Observations**\\n\"\n",
    "    f\"1. Approximately {frac_clv_lost:.1f}% of customer lifetime value is \"\n",
    "    \"lost due to the 16% over the past 12 months in the test data split. The \"\n",
    "    f\"impact to the client is a loss of approximately \"\n",
    "    f\"{clv_per_customer_lost:.1f} dollars of customer lifetime value (per \"\n",
    "    \"customer). Accounting for the cost to CAC, the total loss is \"\n",
    "    f\"{impact_per_cust:,.1f} CAD per customer, or {impact:,.1f} CAD overall, \"\n",
    "    \"due to churn.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda0d22-e089-4cc1-be07-a987502bfef8",
   "metadata": {},
   "source": [
    "### Get Optimal Number of Targeted Customers (`N`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b244c12-1e21-42d9-8099-f37342e2a4ea",
   "metadata": {},
   "source": [
    "Plot true and predicted expected savings and ROI curves to visualize the following\n",
    "\n",
    "1. true ROI\n",
    "2. predicted ROI\n",
    "3. optimal number of customers based on error in predicted ROI\n",
    "\n",
    "using the test data, and compare to the optimal number of customers based on error in predicted ROI using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb059b04-3dff-4826-8ad6-b3370d2e1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vzu.plot_roi_curves(\n",
    "    df_costs_test['n'],\n",
    "    df_costs_test['cum_true_savings'],\n",
    "    df_costs_test['cum_pred_savings'],\n",
    "    df_costs_test['ROI_percent'],\n",
    "    df_costs_test['ROI_percent_pred'],\n",
    "    {\n",
    "        f'Optimal N (validation) = {optimal_N_roi}': {\n",
    "            'x': optimal_N_roi, 'colour': 'black',\n",
    "        },\n",
    "        f'Optimal N (test) = {255}': {\n",
    "            'x': 255, 'colour': 'red',\n",
    "        },\n",
    "    },\n",
    "    ptitle=(\n",
    "        f'Top {255:,} Customers Should be Contacted To Maximize '\n",
    "        'Predicted ROI and Minimize Error in ROI'\n",
    "    ),\n",
    "    legend_loc='lower right',\n",
    "    xlabel=f\"Number of Predicted Churners to Contact (Top-N)\",\n",
    "    ylabel=\"Expected Net Savings ($)\",\n",
    "    fig_size=(12, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24d97e-d8e0-4b5b-9580-c22c9b2dfe63",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "1. This chart shows that 238 is not the optimal number of customers in order to maximize true ROI (green curve). The optimal ROI should be 254, which captures the second peak before the true and predicted ROI diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209bf64-476c-4f21-bfa5-8bceb5e54902",
   "metadata": {},
   "source": [
    "Find the optimal number of customers to target in order to maximize true ROI, using the costs from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749b473-a715-494e-ae7a-a7bf8396565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_optimal_test = (\n",
    "    df_costs_test\n",
    "    .query(\n",
    "        \"(total_intervention_cost > 0) & \"\n",
    "        # capture second peak in ROI\n",
    "        \"(n >= 200) & (n < 300)\"\n",
    "    )\n",
    "    .sort_values(\n",
    "        by=['ROI', 'ROI_error', 'n'], ascending=[False, True, True],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    .head(1)\n",
    ")\n",
    "cols_costs = [\n",
    "    'n',\n",
    "    'cum_true_savings',\n",
    "    'cum_pred_savings',\n",
    "    'clv',\n",
    "    'ROI_error',\n",
    "    'ROI_percent',\n",
    "    'ROI_percent_pred',\n",
    "]\n",
    "optimal_N_roi_test = df_costs_optimal_test['n'].squeeze()\n",
    "(\n",
    "    df_costs_optimal_test[cols_costs].assign(split='test')\n",
    "    .style\n",
    "    .set_properties(\n",
    "        subset=['ROI_percent_pred'],\n",
    "        **{'background-color': 'yellow', 'color': 'black'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def041d-61bb-42e3-9afb-1df12ab5b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_error_optimal = df_costs_optimal_test['ROI_error'].squeeze()\n",
    "true_roi_optimal = df_costs_optimal_test['ROI_percent'].squeeze()\n",
    "predicted_roi_optimal = df_costs_optimal_test['ROI_percent_pred'].squeeze()\n",
    "Markdown(\n",
    "    \"**Observations**\\n\"\n",
    "    \"1. In order to maximize true ROI and minimize the error in true \"\n",
    "    f\"ROI, the optimal number of customers to target is {optimal_N_roi_test}.\\n\"\n",
    "    f\"2. If the top {optimal_N_roi_test} customers from the test data are \"\n",
    "    \"targeted, then the\\n\"\n",
    "    f\"   - error in the predicted ROI is approximately {roi_error_optimal:.1f}%\\n\"\n",
    "    f\"   - true ROI is approximately {true_roi_optimal:.1f}%\\n\"\n",
    "    f\"   - predicted ROI is approximately {predicted_roi_optimal:.1f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf585f5-74db-4ebc-89d6-de638181aeaa",
   "metadata": {},
   "source": [
    "### Get Loss in Predicted ROI Using Optimized Number of Targeted Customers (`N`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c207dc-7b49-406e-9840-ddf3e5fd8755",
   "metadata": {},
   "source": [
    "We must follow the recommendations from model validation that the top 238 predicted churners should be contacted. If we do this, then we can calculate the percent change in the KPI (predicted ROI) compared to this optimal value. This percent change is the predicted loss in ROI that will be incurred by the client by following the recommendations from the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ffc5f-1464-4d2c-a524-19c8daea0b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_test_best = (\n",
    "    df_costs_test[['n', 'y_pred_proba', 'cum_pred_savings', 'ROI_percent_pred']]\n",
    "    .query(f\"(n == {optimal_N_roi}) | (n == {optimal_N_roi_test})\")\n",
    "    .sort_values(by=['n'], ascending=True, ignore_index=True)\n",
    "    .assign(\n",
    "        pct_pred_ROI_lost=lambda df: (\n",
    "            100*df['ROI_percent_pred'].pct_change()\n",
    "        ),\n",
    "        source_for_n=lambda df: pd.Series(\n",
    "            ['validation data', 'test data'], index=df.index\n",
    "        )\n",
    "    )\n",
    ")\n",
    "error_pred_roi = (\n",
    "    df_costs_test_best\n",
    "    .query(\"source_for_n == 'test data'\")\n",
    "    ['pct_pred_ROI_lost']\n",
    "    .squeeze()\n",
    ")\n",
    "display(\n",
    "    df_costs_test_best\n",
    "    .style\n",
    "    .set_properties(\n",
    "        subset=['cum_pred_savings', 'ROI_percent_pred', 'pct_pred_ROI_lost'],\n",
    "        **{'background-color': 'yellow', 'color': 'black'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a028e4-3a00-483c-826a-8deda72c6b95",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "1. Calculations are performed using customers in the unseen (test) data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bca41-abc7-4d00-8a57-27b55f868174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    \"**Observations**\\n\"\n",
    "    \"1. If we apply the recommendations from model validation and contact \"\n",
    "    f\"(target) the top {optimal_N_roi} customers in unseen data (test split), \"\n",
    "    \"then the client is incorrectly reported a loss of approximately \"\n",
    "    f\"{error_pred_roi:.2f}% of the maximum possible predicted ROI.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118ec1a-bb88-4656-8cbe-c044629c8ae8",
   "metadata": {},
   "source": [
    "Append column to costs indicating if targeting customer maximizes ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019bc185-cb23-4c41-af48-40f0209f570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_costs_test = (\n",
    "    df_costs_test\n",
    "    .assign(maximizes_roi=lambda df: df['n'] <= optimal_N_roi)\n",
    ")\n",
    "(\n",
    "    df_costs_test\n",
    "    [\n",
    "        [\n",
    "            'clientnum',\n",
    "            'n',\n",
    "            'y_pred_proba',\n",
    "            'y_pred',\n",
    "            'clv',\n",
    "            'ROI_percent_pred',\n",
    "            'maximizes_roi',\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636c950-85e9-41b5-bd67-e7b9b64067ff",
   "metadata": {},
   "source": [
    "### At-Risk Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d16e0-4dc4-42dc-be9f-8128293c59f0",
   "metadata": {},
   "source": [
    "In order to identify at-risk customers from the `y_pred_proba` (predicted probability) column, we must pick an optimal decision threshold based on the business goal (catching true churners). For the current business use-case, we need to prioritize recall. The optimal decision threshold was determined during ML development.\n",
    "\n",
    "This decision threshold was optimized to maximize F2 Score, since it prioritizes recall over precision, which is in line with the business goal. The tuned threshold is stored in the `best_decision_threshold` column of `df_val_pred` and `df_test_pred`. The `y_pred` column was created by comparing `y_pred_proba` to the best decision threshold. With this in mind, the `y_pred` column already indicates if a customer is at-risk (1) or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b899b45-5b4d-471d-9f85-f1198587b0be",
   "metadata": {},
   "source": [
    "So, the `y_pred` column will now be renamed to `is_at_risk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4aa59-7c1a-428d-a0c4-5059cc6f1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_costs_test = df_costs_test.rename(columns={'y_pred': 'is_at_risk'})\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_costs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b799c-d4ba-4707-bebf-53bb4ef9b379",
   "metadata": {},
   "source": [
    "## Export Project Deliverables to Private R2 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797d3c2-2c47-43e4-8d11-51046569e7af",
   "metadata": {},
   "source": [
    "Get the current timestamp in the format `YYmmdd_HHMMSS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949547f-685a-4764-a016-d8d79a254eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720486e7-dbd1-4799-aff8-80abec61913f",
   "metadata": {},
   "source": [
    "### Unseen Data (Test Split) Customers with Indicator of At-Risk and Maximizing ROI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067bc07-05ea-4c26-b6dd-6c0fd41aabe5",
   "metadata": {},
   "source": [
    "Combine costs (predicted churners) with predicted non-churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60664074-7583-4a1f-9a5a-1fd6bcc6f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test_pred_with_costs = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df_costs_test.assign(y_pred=1),\n",
    "            df_test_pred.query(\"y_pred != 1\"),\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    .fillna(\n",
    "        {\n",
    "            'interchange_rev': np.nan,\n",
    "            'interest_rev': np.nan,\n",
    "            'fee_rev': np.nan,\n",
    "            'annual_rev': np.nan,\n",
    "            'clv': np.nan,\n",
    "            'success_rate': np.nan,\n",
    "            'expected_savings': np.nan,\n",
    "            'true_savings': np.nan,\n",
    "            'cum_pred_savings': np.nan,\n",
    "            'cum_true_savings': np.nan,\n",
    "            'n': np.nan,\n",
    "            'random_savings': np.nan,\n",
    "            'total_intervention_cost': np.nan,\n",
    "            'ROI': np.nan,\n",
    "            'ROI_pred': np.nan,\n",
    "            'ROI_error': np.nan,\n",
    "            'ROI_percent': np.nan,\n",
    "            'ROI_percent_pred': np.nan,\n",
    "            'maximizes_roi': np.nan,\n",
    "            'is_at_risk': 0,\n",
    "        }\n",
    "    )\n",
    "    .convert_dtypes(dtype_backend='pyarrow')\n",
    ")\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_test_pred_with_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dbc39-7f49-4bb0-9420-4282bbdc225d",
   "metadata": {},
   "source": [
    "Next, export to a file in the R2 bucket with the following file name format `test_predictions_with_business_metrics__logisticregression__<current-timestamp-YYmmdd_HHMMSS>.parquet.gzip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64f46b-4623-469b-a009-07b43984d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# export_df_to_r2(\n",
    "#     df_test_pred_with_costs,\n",
    "#     bucket_name,\n",
    "#     (\n",
    "#         f\"test_predictions_with_business_metrics__{best_model_name.lower()}__\"\n",
    "#         f\"{curr_timestamp}.parquet.gzip\"\n",
    "#     ),\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
